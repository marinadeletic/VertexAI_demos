{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3e82d1-322a-46e5-b1d3-23145eea83a8",
   "metadata": {},
   "source": [
    "# VertexAI AutoML with retraining pipeline \n",
    "This notebook creates a MLOps pipeline that takes data from a BQ table, re-trains a vertexAI AutoML Tabluar model and deploys it conditionally to the same endpoint that already exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dd344-e153-49be-b9a9-391ceda59c56",
   "metadata": {},
   "source": [
    "### INSTALLATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d45925-e0a3-4888-be64-ed10aafd012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\"\n",
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.7.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp==1.8.9 google-cloud-pipeline-components==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f134ea-5f15-4acb-be45-5d6438c3c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148572d-c750-4026-9027-b282cd94d151",
   "metadata": {},
   "source": [
    "Check that you have correctly installed the packages. The KFP SDK version should be >=1.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128f156-0a32-4584-9364-d0ce712b2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0aa2f-6766-4bdb-8052-eac85d25eeca",
   "metadata": {},
   "source": [
    "### CREATE BQ DATASET (IF REQUIRED)\n",
    "Creates BQ Dataset called `beans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bacc73-3679-441c-af13-eaf397209cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=us-central1 mk -d \\\n",
    "--description \"Beans Dataset for AUTOML\" \\\n",
    "beans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8291aca-c1c0-48c5-aed5-2aa09a3a27e1",
   "metadata": {},
   "source": [
    "Loads data from CSV to new table called `beans_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e2d59-b9c0-45f2-8760-45daa3e625d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq load \\\n",
    "  --source_format=CSV \\\n",
    "  --autodetect \\\n",
    "  beans.beans_data \\\n",
    "  data/beans_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b7e8a-e3ca-437d-9862-9fda8a9a5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_SOURCE=f\"bq://{PROJECT_ID}.beans.beans_data\"\n",
    "BQ_SOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ac85c-e552-40fc-b9d2-05fa5c8f7ac2",
   "metadata": {},
   "source": [
    "### SET ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344695fc-0c46-4fb7-8d7f-34f17da69f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "PROJECT_ID = \"marinadel\"\n",
    "BUCKET_NAME=\"gs://marinadel-bucket\"\n",
    "BQ_SOURCE=\"bq://marinadel.beans.beans_data\"  ##bq://{PROJECT_ID}.{DATASET}.{TABLE} Continue to next step if you havent set one up \n",
    "BUILD_NAME = 'automl-beans' \n",
    "DISPLAY_NAME = 'automl-beans-{}'.format(str(int(time.time())))  ## timestamped name for run \n",
    "\n",
    "REGION=\"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22841644-bbf5-4395-b400-c125ff773e84",
   "metadata": {},
   "source": [
    "### DEFINE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca228e4e-5d64-4943-986f-532e0c161b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846896e9-4a04-4a7e-b303-04521acead0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"tabular_eval_component.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def classification_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"us-central1\",\n",
    "    api_endpoint: str,  # \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform as aip\n",
    "\n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(client, model_name):\n",
    "        from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is\n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    logging.info(\"{} < {}; returning False\".format(metrics_dict[k], v))\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aip.init(project=project)\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aip.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\n",
    "        client, model_resource_path\n",
    "    )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5345852c-ad61-4d74-a154-8e462e33081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:latest\",\n",
    "    output_component_file=\"replace_model_on_endpoint.yaml\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "\n",
    "# Creates an enpoint if none exist for this pipeline, else uses the old one \n",
    "def replace_model_on_endpoint(project: str, display_name: str, model_name: str, traffic_percentage: int ):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as aip\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    models= aip.Model.list(filter=f\"display_name={model_name}\", project = project)\n",
    "    model = models[0] \n",
    "    print(\"model to deploy: \", models[0]._gca_resource.name)\n",
    "\n",
    "    print(f\"Looking for endpoints\")\n",
    "    endpoints = aip.Endpoint.list(filter=f\"display_name={display_name}\", project = project ) \n",
    "\n",
    "    if endpoints == []:\n",
    "        print(f\"No reusable endpoint found, creating new endpoint: {display_name}\")\n",
    "        endpoint = aip.Endpoint.create(f\"{display_name}\", project = project)\n",
    "        print(f\"endpoint_uri = {endpoint._gca_resource.name}\")\n",
    "        print(f\"deploying model\")\n",
    "        model.deploy(endpoint= endpoint, \n",
    "                 machine_type = \"n1-standard-4\" , \n",
    "                 min_replica_count=1,\n",
    "                 max_replica_count=1)\n",
    "    else:\n",
    "        endpoint = endpoints[0]\n",
    "        print(f\"Reusable endpoint found. Endpoint_id: {endpoint}\")\n",
    "        print(f\"endpoint_uri = {endpoint._gca_resource.name}\")\n",
    "        print(f\"deploying model with traffic_percentage= {traffic_percentage}\")\n",
    "        model.deploy(endpoint= endpoint,\n",
    "                     traffic_percentage = traffic_percentage,\n",
    "                     machine_type = \"n1-standard-4\" , \n",
    "                     min_replica_count=1,\n",
    "                     max_replica_count=1) \n",
    "\n",
    "\n",
    "    print(f\"model deployed\")\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cc2ea2-90dc-4a93-b8f0-4ed952d5a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=f\"{BUILD_NAME}-pipeline\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = BQ_SOURCE,\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    build_name: str = BUILD_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "    trafic_split: int = 50 \n",
    "):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=display_name, bq_source=bq_source\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=100,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\",\n",
    "    )\n",
    "\n",
    "\n",
    "    model_eval_task = classification_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    with dsl.Condition(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        endpoint_op = replace_model_on_endpoint(project, f\"{build_name}-endpoint\",  \n",
    "                                                display_name, trafic_split )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5c34bd-9edb-4cc5-a3ed-cc7908c7a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"automl_rep_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561e5c7b-aa53-4af1-9dad-8338dd2aab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{BUILD_NAME}-training\",\n",
    "    template_path=\"automl_rep_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID, \"display_name\": DISPLAY_NAME},\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98c11f7-de40-4052-8824-b5e73f0f84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/865822053282/locations/us-central1/pipelineJobs/automl-beans-pipeline-20220317211001\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/865822053282/locations/us-central1/pipelineJobs/automl-beans-pipeline-20220317211001')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-beans-pipeline-20220317211001?project=865822053282\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a3e13-654c-433e-9bdc-263f80a3e9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
